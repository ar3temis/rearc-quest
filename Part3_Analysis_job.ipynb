{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\n\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session  \n\njob = Job(glueContext)\njob.init(\"analysis_job\")\n\n# Define S3 paths\ns3_bucket = \"s3://bls-gov-dataset/\"\ncsv_path = s3_bucket + \"bls-data/pr.data.0.Current\"\njson_path = s3_bucket + \"datausa_population.json\"\n\n# Load CSV data into DataFrame\ndf_csv = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"\\t\").csv(csv_path)\n\n# Strip leading and trailing spaces from column names\ndf_csv = df_csv.toDF(*[c.strip() for c in df_csv.columns])\n\n# Apply trim to all columns\ndf_csv = df_csv.select([trim(col(c)).alias(c) for c in df_csv.columns])\n\n# Cast year to int\ndf_csv = df_csv.withColumn(\"year\", col(\"year\").cast(\"int\"))\n\n# Load JSON data into DataFrame\ndf_json = spark.read.option(\"multiline\", \"true\").json(json_path)\n\n# Part 3.1: Calculate mean and standard deviation of annual US population (2013-2018)\ndf_population_filtered = df_json.filter((col(\"Year\") >= 2013) & (col(\"Year\") <= 2018))\ndf_population_stats = df_population_filtered.select(\n    mean(\"Population\").alias(\"Mean_Population\"),\n    stddev(\"Population\").alias(\"StdDev_Population\")\n)\ndf_population_stats.show()\n\n# Part 3.2: Find the best year for each series_id based on the largest annual sum of 'value' using SQL\ndf_csv.createOrReplaceTempView(\"csv_table\")\ndf_best_year = spark.sql(\"\"\"\n    WITH annual_values AS (\n        SELECT series_id, CAST(year AS INT) AS year, SUM(value) AS annual_value_sum\n        FROM csv_table\n        GROUP BY series_id, year\n    ), ranked_values AS (\n        SELECT *, ROW_NUMBER() OVER (PARTITION BY series_id ORDER BY annual_value_sum DESC) AS rank\n        FROM annual_values\n    )\n    SELECT series_id, year, ceil(round(annual_value_sum)) as max_value FROM ranked_values WHERE rank = 1\n\"\"\")\ndf_best_year.show()\n\n# Part 3.3: Report for series_id = 'PRS30006032' and period = 'Q01' with corresponding population\ndf_filtered_series = df_csv.filter((col(\"series_id\") == \"PRS30006032\") & (col(\"period\") == \"Q01\"))\ndf_joined = df_filtered_series.join(df_json, df_filtered_series.year == df_json.Year, \"left\").select(\n    df_filtered_series.series_id,\n    df_filtered_series.year,\n    df_filtered_series.period,\n    df_filtered_series.value,\n    df_json.Population)\ndf_joined = df_joined.filter(col(\"Population\").isNotNull())\ndf_joined.show()\n\njob.commit()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}
