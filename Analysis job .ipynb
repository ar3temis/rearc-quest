{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"Analysis\").getOrCreate()\n\n# Define S3 paths\ns3_bucket = \"s3://bls-gov-dataset/\"\ncsv_path = s3_bucket + \"bls-data/pr.data.0.Current\"\njson_path = s3_bucket + \"datausa_population.json\"\n\n# Load CSV data into DataFrame\ndf_csv = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"\\t\").csv(csv_path)\n\n# Strip leading and trailing spaces from column names\ndf_csv = df_csv.toDF(*[c.strip() for c in df_csv.columns])\n\n# Apply trim to all columns\ndf_csv = df_csv.select([trim(col(c)).alias(c) for c in df_csv.columns])\n\n# Cast year to int\ndf_csv = df_csv.withColumn(\"year\", col(\"year\").cast(\"int\"))\n\n# Load JSON data into DataFrame\ndf_json = spark.read.option(\"multiline\", \"true\").json(json_path)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Part 3.1: Calculate mean and standard deviation of annual US population (2013-2018)\ndf_population_filtered = df_json.filter((col(\"Year\") >= 2013) & (col(\"Year\") <= 2018))\ndf_population_stats = df_population_filtered.select(\n    mean(\"Population\").alias(\"Mean_Population\"),\n    stddev(\"Population\").alias(\"StdDev_Population\")\n)\ndf_population_stats.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------------+-----------------+\n|Mean_Population|StdDev_Population|\n+---------------+-----------------+\n|   3.17437383E8| 4257089.54152933|\n+---------------+-----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Part 3.2: Find the best year for each series_id based on the largest annual sum of 'value' using SQL\ndf_csv.createOrReplaceTempView(\"csv_table\")\ndf_best_year = spark.sql(\"\"\"\n    WITH annual_values AS (\n        SELECT series_id, CAST(year AS INT) AS year, SUM(value) AS annual_value_sum\n        FROM csv_table\n        GROUP BY series_id, year\n    ), ranked_values AS (\n        SELECT *, ROW_NUMBER() OVER (PARTITION BY series_id ORDER BY annual_value_sum DESC) AS rank\n        FROM annual_values\n    )\n    SELECT series_id, year, ceil(round(annual_value_sum)) as max_value FROM ranked_values WHERE rank = 1\n\"\"\")\ndf_best_year.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+----+---------+\n|  series_id|year|max_value|\n+-----------+----+---------+\n|PRS30006011|2022|       21|\n|PRS30006012|2022|       17|\n|PRS30006013|1998|      704|\n|PRS30006021|2010|       18|\n|PRS30006022|2010|       13|\n|PRS30006023|2014|      503|\n|PRS30006031|2022|       20|\n|PRS30006032|2021|       17|\n|PRS30006033|1998|      701|\n|PRS30006061|2022|       37|\n|PRS30006062|2021|       32|\n|PRS30006063|2024|      646|\n|PRS30006081|2021|       24|\n|PRS30006082|2021|       24|\n|PRS30006083|2021|      111|\n|PRS30006091|2002|       43|\n|PRS30006092|2002|       45|\n|PRS30006093|2013|      515|\n|PRS30006101|2020|       33|\n|PRS30006102|2020|       37|\n+-----------+----+---------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Part 3.3: Report for series_id = 'PRS30006032' and period = 'Q01' with corresponding population\ndf_filtered_series = df_csv.filter((col(\"series_id\") == \"PRS30006032\") & (col(\"period\") == \"Q01\"))\ndf_joined = df_filtered_series.join(df_json, df_filtered_series.year == df_json.Year, \"left\").select(\n    df_filtered_series.series_id,\n    df_filtered_series.year,\n    df_filtered_series.period,\n    df_filtered_series.value,\n    df_json.Population)\ndf_joined = df_joined.filter(col(\"Population\").isNotNull())\ndf_joined.show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+----+------+-----+----------+\n|  series_id|year|period|value|Population|\n+-----------+----+------+-----+----------+\n|PRS30006032|2013|   Q01|  0.8| 311536594|\n|PRS30006032|2014|   Q01| -0.1| 314107084|\n|PRS30006032|2015|   Q01| -1.6| 316515021|\n|PRS30006032|2016|   Q01| -1.4| 318558162|\n|PRS30006032|2017|   Q01|  0.7| 321004407|\n|PRS30006032|2018|   Q01|  0.4| 322903030|\n|PRS30006032|2019|   Q01| -1.6| 324697795|\n|PRS30006032|2020|   Q01| -6.7| 326569308|\n|PRS30006032|2021|   Q01|  1.2| 329725481|\n|PRS30006032|2022|   Q01|  5.6| 331097593|\n+-----------+----+------+-----+----------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Stop the Spark session\nspark.stop()",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}